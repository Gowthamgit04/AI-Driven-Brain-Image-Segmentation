{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f4212e4-4729-4413-8aa1-ff47d4e527b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from glob import glob\n",
    "from sklearn.utils import shuffle\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger, ReduceLROnPlateau, EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from sklearn.model_selection import train_test_split\n",
    "from patchify import patchify\n",
    "from tqdm import tqdm\n",
    "from math import log2\n",
    "import tensorflow.keras.layers as L\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import get_custom_objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e8b94d7-3df8-43e2-8ad9-b4c45baccf79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install patchify\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39e7f476-9f24-4b37-8e1b-a4c3db6a461c",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 1e-12\n",
    "def d_coef(y_actual, y_pred):\n",
    "    y_actual = tf.keras.layers.Flatten()(y_actual)\n",
    "    y_pred = tf.keras.layers.Flatten()(y_pred)\n",
    "    inter = tf.reduce_sum(y_actual * y_pred)\n",
    "    return (2. * inter + m) / (tf.reduce_sum(y_actual) + tf.reduce_sum(y_pred) + m)\n",
    "\n",
    "def d_loss(y_actual, y_pred):\n",
    "    return 1.0 - d_coef(y_actual, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47521eda-1413-4932-b583-f83836920302",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"UNETR_2D\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 262144, 3)]          0         []                            \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 262144, 64)           256       ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " tf.__operators__.add (TFOp  (None, 262144, 64)           0         ['dense[0][0]']               \n",
      " Lambda)                                                                                          \n",
      "                                                                                                  \n",
      " layer_normalization (Layer  (None, 262144, 64)           128       ['tf.__operators__.add[0][0]']\n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " multi_head_attention (Mult  (None, 262144, 64)           99520     ['layer_normalization[0][0]', \n",
      " iHeadAttention)                                                     'layer_normalization[0][0]'] \n",
      "                                                                                                  \n",
      " add (Add)                   (None, 262144, 64)           0         ['multi_head_attention[0][0]',\n",
      "                                                                     'tf.__operators__.add[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_1 (Lay  (None, 262144, 64)           128       ['add[0][0]']                 \n",
      " erNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " dense_1 (Dense)             (None, 262144, 128)          8320      ['layer_normalization_1[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " dropout (Dropout)           (None, 262144, 128)          0         ['dense_1[0][0]']             \n",
      "                                                                                                  \n",
      " dense_2 (Dense)             (None, 262144, 64)           8256      ['dropout[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)         (None, 262144, 64)           0         ['dense_2[0][0]']             \n",
      "                                                                                                  \n",
      " add_1 (Add)                 (None, 262144, 64)           0         ['dropout_1[0][0]',           \n",
      "                                                                     'add[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_2 (Lay  (None, 262144, 64)           128       ['add_1[0][0]']               \n",
      " erNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " multi_head_attention_1 (Mu  (None, 262144, 64)           99520     ['layer_normalization_2[0][0]'\n",
      " ltiHeadAttention)                                                  , 'layer_normalization_2[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " add_2 (Add)                 (None, 262144, 64)           0         ['multi_head_attention_1[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'add_1[0][0]']               \n",
      "                                                                                                  \n",
      " layer_normalization_3 (Lay  (None, 262144, 64)           128       ['add_2[0][0]']               \n",
      " erNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " dense_3 (Dense)             (None, 262144, 128)          8320      ['layer_normalization_3[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)         (None, 262144, 128)          0         ['dense_3[0][0]']             \n",
      "                                                                                                  \n",
      " dense_4 (Dense)             (None, 262144, 64)           8256      ['dropout_2[0][0]']           \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)         (None, 262144, 64)           0         ['dense_4[0][0]']             \n",
      "                                                                                                  \n",
      " add_3 (Add)                 (None, 262144, 64)           0         ['dropout_3[0][0]',           \n",
      "                                                                     'add_2[0][0]']               \n",
      "                                                                                                  \n",
      " layer_normalization_4 (Lay  (None, 262144, 64)           128       ['add_3[0][0]']               \n",
      " erNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " multi_head_attention_2 (Mu  (None, 262144, 64)           99520     ['layer_normalization_4[0][0]'\n",
      " ltiHeadAttention)                                                  , 'layer_normalization_4[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " add_4 (Add)                 (None, 262144, 64)           0         ['multi_head_attention_2[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'add_3[0][0]']               \n",
      "                                                                                                  \n",
      " layer_normalization_5 (Lay  (None, 262144, 64)           128       ['add_4[0][0]']               \n",
      " erNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " dense_5 (Dense)             (None, 262144, 128)          8320      ['layer_normalization_5[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)         (None, 262144, 128)          0         ['dense_5[0][0]']             \n",
      "                                                                                                  \n",
      " dense_6 (Dense)             (None, 262144, 64)           8256      ['dropout_4[0][0]']           \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)         (None, 262144, 64)           0         ['dense_6[0][0]']             \n",
      "                                                                                                  \n",
      " add_5 (Add)                 (None, 262144, 64)           0         ['dropout_5[0][0]',           \n",
      "                                                                     'add_4[0][0]']               \n",
      "                                                                                                  \n",
      " layer_normalization_6 (Lay  (None, 262144, 64)           128       ['add_5[0][0]']               \n",
      " erNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " multi_head_attention_3 (Mu  (None, 262144, 64)           99520     ['layer_normalization_6[0][0]'\n",
      " ltiHeadAttention)                                                  , 'layer_normalization_6[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " add_6 (Add)                 (None, 262144, 64)           0         ['multi_head_attention_3[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'add_5[0][0]']               \n",
      "                                                                                                  \n",
      " layer_normalization_7 (Lay  (None, 262144, 64)           128       ['add_6[0][0]']               \n",
      " erNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " dense_7 (Dense)             (None, 262144, 128)          8320      ['layer_normalization_7[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)         (None, 262144, 128)          0         ['dense_7[0][0]']             \n",
      "                                                                                                  \n",
      " dense_8 (Dense)             (None, 262144, 64)           8256      ['dropout_6[0][0]']           \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)         (None, 262144, 64)           0         ['dense_8[0][0]']             \n",
      "                                                                                                  \n",
      " add_7 (Add)                 (None, 262144, 64)           0         ['dropout_7[0][0]',           \n",
      "                                                                     'add_6[0][0]']               \n",
      "                                                                                                  \n",
      " layer_normalization_8 (Lay  (None, 262144, 64)           128       ['add_7[0][0]']               \n",
      " erNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " multi_head_attention_4 (Mu  (None, 262144, 64)           99520     ['layer_normalization_8[0][0]'\n",
      " ltiHeadAttention)                                                  , 'layer_normalization_8[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " add_8 (Add)                 (None, 262144, 64)           0         ['multi_head_attention_4[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'add_7[0][0]']               \n",
      "                                                                                                  \n",
      " layer_normalization_9 (Lay  (None, 262144, 64)           128       ['add_8[0][0]']               \n",
      " erNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " dense_9 (Dense)             (None, 262144, 128)          8320      ['layer_normalization_9[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)         (None, 262144, 128)          0         ['dense_9[0][0]']             \n",
      "                                                                                                  \n",
      " dense_10 (Dense)            (None, 262144, 64)           8256      ['dropout_8[0][0]']           \n",
      "                                                                                                  \n",
      " dropout_9 (Dropout)         (None, 262144, 64)           0         ['dense_10[0][0]']            \n",
      "                                                                                                  \n",
      " add_9 (Add)                 (None, 262144, 64)           0         ['dropout_9[0][0]',           \n",
      "                                                                     'add_8[0][0]']               \n",
      "                                                                                                  \n",
      " layer_normalization_10 (La  (None, 262144, 64)           128       ['add_9[0][0]']               \n",
      " yerNormalization)                                                                                \n",
      "                                                                                                  \n",
      " multi_head_attention_5 (Mu  (None, 262144, 64)           99520     ['layer_normalization_10[0][0]\n",
      " ltiHeadAttention)                                                  ',                            \n",
      "                                                                     'layer_normalization_10[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " add_10 (Add)                (None, 262144, 64)           0         ['multi_head_attention_5[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'add_9[0][0]']               \n",
      "                                                                                                  \n",
      " layer_normalization_11 (La  (None, 262144, 64)           128       ['add_10[0][0]']              \n",
      " yerNormalization)                                                                                \n",
      "                                                                                                  \n",
      " dense_11 (Dense)            (None, 262144, 128)          8320      ['layer_normalization_11[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_10 (Dropout)        (None, 262144, 128)          0         ['dense_11[0][0]']            \n",
      "                                                                                                  \n",
      " dense_12 (Dense)            (None, 262144, 64)           8256      ['dropout_10[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_11 (Dropout)        (None, 262144, 64)           0         ['dense_12[0][0]']            \n",
      "                                                                                                  \n",
      " add_11 (Add)                (None, 262144, 64)           0         ['dropout_11[0][0]',          \n",
      "                                                                     'add_10[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_12 (La  (None, 262144, 64)           128       ['add_11[0][0]']              \n",
      " yerNormalization)                                                                                \n",
      "                                                                                                  \n",
      " multi_head_attention_6 (Mu  (None, 262144, 64)           99520     ['layer_normalization_12[0][0]\n",
      " ltiHeadAttention)                                                  ',                            \n",
      "                                                                     'layer_normalization_12[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " add_12 (Add)                (None, 262144, 64)           0         ['multi_head_attention_6[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'add_11[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_13 (La  (None, 262144, 64)           128       ['add_12[0][0]']              \n",
      " yerNormalization)                                                                                \n",
      "                                                                                                  \n",
      " dense_13 (Dense)            (None, 262144, 128)          8320      ['layer_normalization_13[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_12 (Dropout)        (None, 262144, 128)          0         ['dense_13[0][0]']            \n",
      "                                                                                                  \n",
      " dense_14 (Dense)            (None, 262144, 64)           8256      ['dropout_12[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_13 (Dropout)        (None, 262144, 64)           0         ['dense_14[0][0]']            \n",
      "                                                                                                  \n",
      " add_13 (Add)                (None, 262144, 64)           0         ['dropout_13[0][0]',          \n",
      "                                                                     'add_12[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_14 (La  (None, 262144, 64)           128       ['add_13[0][0]']              \n",
      " yerNormalization)                                                                                \n",
      "                                                                                                  \n",
      " multi_head_attention_7 (Mu  (None, 262144, 64)           99520     ['layer_normalization_14[0][0]\n",
      " ltiHeadAttention)                                                  ',                            \n",
      "                                                                     'layer_normalization_14[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " add_14 (Add)                (None, 262144, 64)           0         ['multi_head_attention_7[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'add_13[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_15 (La  (None, 262144, 64)           128       ['add_14[0][0]']              \n",
      " yerNormalization)                                                                                \n",
      "                                                                                                  \n",
      " dense_15 (Dense)            (None, 262144, 128)          8320      ['layer_normalization_15[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_14 (Dropout)        (None, 262144, 128)          0         ['dense_15[0][0]']            \n",
      "                                                                                                  \n",
      " dense_16 (Dense)            (None, 262144, 64)           8256      ['dropout_14[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_15 (Dropout)        (None, 262144, 64)           0         ['dense_16[0][0]']            \n",
      "                                                                                                  \n",
      " add_15 (Add)                (None, 262144, 64)           0         ['dropout_15[0][0]',          \n",
      "                                                                     'add_14[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_16 (La  (None, 262144, 64)           128       ['add_15[0][0]']              \n",
      " yerNormalization)                                                                                \n",
      "                                                                                                  \n",
      " multi_head_attention_8 (Mu  (None, 262144, 64)           99520     ['layer_normalization_16[0][0]\n",
      " ltiHeadAttention)                                                  ',                            \n",
      "                                                                     'layer_normalization_16[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " add_16 (Add)                (None, 262144, 64)           0         ['multi_head_attention_8[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'add_15[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_17 (La  (None, 262144, 64)           128       ['add_16[0][0]']              \n",
      " yerNormalization)                                                                                \n",
      "                                                                                                  \n",
      " dense_17 (Dense)            (None, 262144, 128)          8320      ['layer_normalization_17[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_16 (Dropout)        (None, 262144, 128)          0         ['dense_17[0][0]']            \n",
      "                                                                                                  \n",
      " dense_18 (Dense)            (None, 262144, 64)           8256      ['dropout_16[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_17 (Dropout)        (None, 262144, 64)           0         ['dense_18[0][0]']            \n",
      "                                                                                                  \n",
      " add_17 (Add)                (None, 262144, 64)           0         ['dropout_17[0][0]',          \n",
      "                                                                     'add_16[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_18 (La  (None, 262144, 64)           128       ['add_17[0][0]']              \n",
      " yerNormalization)                                                                                \n",
      "                                                                                                  \n",
      " multi_head_attention_9 (Mu  (None, 262144, 64)           99520     ['layer_normalization_18[0][0]\n",
      " ltiHeadAttention)                                                  ',                            \n",
      "                                                                     'layer_normalization_18[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " add_18 (Add)                (None, 262144, 64)           0         ['multi_head_attention_9[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'add_17[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_19 (La  (None, 262144, 64)           128       ['add_18[0][0]']              \n",
      " yerNormalization)                                                                                \n",
      "                                                                                                  \n",
      " dense_19 (Dense)            (None, 262144, 128)          8320      ['layer_normalization_19[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_18 (Dropout)        (None, 262144, 128)          0         ['dense_19[0][0]']            \n",
      "                                                                                                  \n",
      " dense_20 (Dense)            (None, 262144, 64)           8256      ['dropout_18[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_19 (Dropout)        (None, 262144, 64)           0         ['dense_20[0][0]']            \n",
      "                                                                                                  \n",
      " add_19 (Add)                (None, 262144, 64)           0         ['dropout_19[0][0]',          \n",
      "                                                                     'add_18[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_20 (La  (None, 262144, 64)           128       ['add_19[0][0]']              \n",
      " yerNormalization)                                                                                \n",
      "                                                                                                  \n",
      " multi_head_attention_10 (M  (None, 262144, 64)           99520     ['layer_normalization_20[0][0]\n",
      " ultiHeadAttention)                                                 ',                            \n",
      "                                                                     'layer_normalization_20[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " add_20 (Add)                (None, 262144, 64)           0         ['multi_head_attention_10[0][0\n",
      "                                                                    ]',                           \n",
      "                                                                     'add_19[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_21 (La  (None, 262144, 64)           128       ['add_20[0][0]']              \n",
      " yerNormalization)                                                                                \n",
      "                                                                                                  \n",
      " dense_21 (Dense)            (None, 262144, 128)          8320      ['layer_normalization_21[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_20 (Dropout)        (None, 262144, 128)          0         ['dense_21[0][0]']            \n",
      "                                                                                                  \n",
      " dense_22 (Dense)            (None, 262144, 64)           8256      ['dropout_20[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_21 (Dropout)        (None, 262144, 64)           0         ['dense_22[0][0]']            \n",
      "                                                                                                  \n",
      " add_21 (Add)                (None, 262144, 64)           0         ['dropout_21[0][0]',          \n",
      "                                                                     'add_20[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_22 (La  (None, 262144, 64)           128       ['add_21[0][0]']              \n",
      " yerNormalization)                                                                                \n",
      "                                                                                                  \n",
      " multi_head_attention_11 (M  (None, 262144, 64)           99520     ['layer_normalization_22[0][0]\n",
      " ultiHeadAttention)                                                 ',                            \n",
      "                                                                     'layer_normalization_22[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " add_22 (Add)                (None, 262144, 64)           0         ['multi_head_attention_11[0][0\n",
      "                                                                    ]',                           \n",
      "                                                                     'add_21[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_23 (La  (None, 262144, 64)           128       ['add_22[0][0]']              \n",
      " yerNormalization)                                                                                \n",
      "                                                                                                  \n",
      " dense_23 (Dense)            (None, 262144, 128)          8320      ['layer_normalization_23[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_22 (Dropout)        (None, 262144, 128)          0         ['dense_23[0][0]']            \n",
      "                                                                                                  \n",
      " dense_24 (Dense)            (None, 262144, 64)           8256      ['dropout_22[0][0]']          \n",
      "                                                                                                  \n",
      " reshape_3 (Reshape)         (None, 512, 512, 64)         0         ['add_17[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_23 (Dropout)        (None, 262144, 64)           0         ['dense_24[0][0]']            \n",
      "                                                                                                  \n",
      " max_pooling2d_2 (MaxPoolin  (None, 32, 32, 64)           0         ['reshape_3[0][0]']           \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " add_23 (Add)                (None, 262144, 64)           0         ['dropout_23[0][0]',          \n",
      "                                                                     'add_22[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_transpose_1 (Conv2D  (None, 64, 64, 128)          32896     ['max_pooling2d_2[0][0]']     \n",
      " Transpose)                                                                                       \n",
      "                                                                                                  \n",
      " reshape_4 (Reshape)         (None, 512, 512, 64)         0         ['add_23[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)             (None, 64, 64, 128)          147584    ['conv2d_transpose_1[0][0]']  \n",
      "                                                                                                  \n",
      " max_pooling2d_3 (MaxPoolin  (None, 32, 32, 64)           0         ['reshape_4[0][0]']           \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " batch_normalization (Batch  (None, 64, 64, 128)          512       ['conv2d[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " conv2d_transpose (Conv2DTr  (None, 64, 64, 128)          32896     ['max_pooling2d_3[0][0]']     \n",
      " anspose)                                                                                         \n",
      "                                                                                                  \n",
      " re_lu (ReLU)                (None, 64, 64, 128)          0         ['batch_normalization[0][0]'] \n",
      "                                                                                                  \n",
      " reshape_2 (Reshape)         (None, 512, 512, 64)         0         ['add_11[0][0]']              \n",
      "                                                                                                  \n",
      " tf.math.multiply (TFOpLamb  (None, 64, 64, 128)          0         ['conv2d_transpose[0][0]']    \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.math.multiply_1 (TFOpLa  (None, 64, 64, 128)          0         ['re_lu[0][0]']               \n",
      " mbda)                                                                                            \n",
      "                                                                                                  \n",
      " max_pooling2d_1 (MaxPoolin  (None, 32, 32, 64)           0         ['reshape_2[0][0]']           \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " add_24 (Add)                (None, 64, 64, 128)          0         ['tf.math.multiply[0][0]',    \n",
      "                                                                     'tf.math.multiply_1[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_transpose_3 (Conv2D  (None, 64, 64, 64)           16448     ['max_pooling2d_1[0][0]']     \n",
      " Transpose)                                                                                       \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)           (None, 64, 64, 128)          147584    ['add_24[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)           (None, 64, 64, 64)           36928     ['conv2d_transpose_3[0][0]']  \n",
      "                                                                                                  \n",
      " batch_normalization_1 (Bat  (None, 64, 64, 128)          512       ['conv2d_1[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_3 (Bat  (None, 64, 64, 64)           256       ['conv2d_3[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " re_lu_1 (ReLU)              (None, 64, 64, 128)          0         ['batch_normalization_1[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " re_lu_3 (ReLU)              (None, 64, 64, 64)           0         ['batch_normalization_3[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " reshape_1 (Reshape)         (None, 512, 512, 64)         0         ['add_5[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)           (None, 64, 64, 128)          147584    ['re_lu_1[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_transpose_4 (Conv2D  (None, 128, 128, 64)         16448     ['re_lu_3[0][0]']             \n",
      " Transpose)                                                                                       \n",
      "                                                                                                  \n",
      " max_pooling2d (MaxPooling2  (None, 32, 32, 64)           0         ['reshape_1[0][0]']           \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " batch_normalization_2 (Bat  (None, 64, 64, 128)          512       ['conv2d_2[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)           (None, 128, 128, 64)         36928     ['conv2d_transpose_4[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_transpose_6 (Conv2D  (None, 64, 64, 32)           8224      ['max_pooling2d[0][0]']       \n",
      " Transpose)                                                                                       \n",
      "                                                                                                  \n",
      " re_lu_2 (ReLU)              (None, 64, 64, 128)          0         ['batch_normalization_2[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " batch_normalization_4 (Bat  (None, 128, 128, 64)         256       ['conv2d_4[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)           (None, 64, 64, 32)           9248      ['conv2d_transpose_6[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_transpose_2 (Conv2D  (None, 128, 128, 64)         32832     ['re_lu_2[0][0]']             \n",
      " Transpose)                                                                                       \n",
      "                                                                                                  \n",
      " re_lu_4 (ReLU)              (None, 128, 128, 64)         0         ['batch_normalization_4[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " batch_normalization_7 (Bat  (None, 64, 64, 32)           128       ['conv2d_7[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " tf.math.multiply_2 (TFOpLa  (None, 128, 128, 64)         0         ['conv2d_transpose_2[0][0]']  \n",
      " mbda)                                                                                            \n",
      "                                                                                                  \n",
      " tf.math.multiply_3 (TFOpLa  (None, 128, 128, 64)         0         ['re_lu_4[0][0]']             \n",
      " mbda)                                                                                            \n",
      "                                                                                                  \n",
      " re_lu_7 (ReLU)              (None, 64, 64, 32)           0         ['batch_normalization_7[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " add_25 (Add)                (None, 128, 128, 64)         0         ['tf.math.multiply_2[0][0]',  \n",
      "                                                                     'tf.math.multiply_3[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_transpose_7 (Conv2D  (None, 128, 128, 32)         4128      ['re_lu_7[0][0]']             \n",
      " Transpose)                                                                                       \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)           (None, 128, 128, 64)         36928     ['add_25[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)           (None, 128, 128, 32)         9248      ['conv2d_transpose_7[0][0]']  \n",
      "                                                                                                  \n",
      " batch_normalization_5 (Bat  (None, 128, 128, 64)         256       ['conv2d_5[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_8 (Bat  (None, 128, 128, 32)         128       ['conv2d_8[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " re_lu_5 (ReLU)              (None, 128, 128, 64)         0         ['batch_normalization_5[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " re_lu_8 (ReLU)              (None, 128, 128, 32)         0         ['batch_normalization_8[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)           (None, 128, 128, 64)         36928     ['re_lu_5[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_transpose_8 (Conv2D  (None, 256, 256, 32)         4128      ['re_lu_8[0][0]']             \n",
      " Transpose)                                                                                       \n",
      "                                                                                                  \n",
      " batch_normalization_6 (Bat  (None, 128, 128, 64)         256       ['conv2d_6[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " conv2d_9 (Conv2D)           (None, 256, 256, 32)         9248      ['conv2d_transpose_8[0][0]']  \n",
      "                                                                                                  \n",
      " re_lu_6 (ReLU)              (None, 128, 128, 64)         0         ['batch_normalization_6[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " batch_normalization_9 (Bat  (None, 256, 256, 32)         128       ['conv2d_9[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " conv2d_transpose_5 (Conv2D  (None, 256, 256, 32)         8224      ['re_lu_6[0][0]']             \n",
      " Transpose)                                                                                       \n",
      "                                                                                                  \n",
      " re_lu_9 (ReLU)              (None, 256, 256, 32)         0         ['batch_normalization_9[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " tf.math.multiply_4 (TFOpLa  (None, 256, 256, 32)         0         ['conv2d_transpose_5[0][0]']  \n",
      " mbda)                                                                                            \n",
      "                                                                                                  \n",
      " tf.math.multiply_5 (TFOpLa  (None, 256, 256, 32)         0         ['re_lu_9[0][0]']             \n",
      " mbda)                                                                                            \n",
      "                                                                                                  \n",
      " add_26 (Add)                (None, 256, 256, 32)         0         ['tf.math.multiply_4[0][0]',  \n",
      "                                                                     'tf.math.multiply_5[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_10 (Conv2D)          (None, 256, 256, 32)         9248      ['add_26[0][0]']              \n",
      "                                                                                                  \n",
      " reshape (Reshape)           (None, 512, 512, 3)          0         ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_10 (Ba  (None, 256, 256, 32)         128       ['conv2d_10[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " conv2d_12 (Conv2D)          (None, 512, 512, 16)         448       ['reshape[0][0]']             \n",
      "                                                                                                  \n",
      " re_lu_10 (ReLU)             (None, 256, 256, 32)         0         ['batch_normalization_10[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " batch_normalization_12 (Ba  (None, 512, 512, 16)         64        ['conv2d_12[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " conv2d_11 (Conv2D)          (None, 256, 256, 32)         9248      ['re_lu_10[0][0]']            \n",
      "                                                                                                  \n",
      " re_lu_12 (ReLU)             (None, 512, 512, 16)         0         ['batch_normalization_12[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " batch_normalization_11 (Ba  (None, 256, 256, 32)         128       ['conv2d_11[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " conv2d_13 (Conv2D)          (None, 512, 512, 16)         2320      ['re_lu_12[0][0]']            \n",
      "                                                                                                  \n",
      " re_lu_11 (ReLU)             (None, 256, 256, 32)         0         ['batch_normalization_11[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " batch_normalization_13 (Ba  (None, 512, 512, 16)         64        ['conv2d_13[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " conv2d_transpose_9 (Conv2D  (None, 512, 512, 16)         2064      ['re_lu_11[0][0]']            \n",
      " Transpose)                                                                                       \n",
      "                                                                                                  \n",
      " re_lu_13 (ReLU)             (None, 512, 512, 16)         0         ['batch_normalization_13[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " tf.math.multiply_6 (TFOpLa  (None, 512, 512, 16)         0         ['conv2d_transpose_9[0][0]']  \n",
      " mbda)                                                                                            \n",
      "                                                                                                  \n",
      " tf.math.multiply_7 (TFOpLa  (None, 512, 512, 16)         0         ['re_lu_13[0][0]']            \n",
      " mbda)                                                                                            \n",
      "                                                                                                  \n",
      " add_27 (Add)                (None, 512, 512, 16)         0         ['tf.math.multiply_6[0][0]',  \n",
      "                                                                     'tf.math.multiply_7[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_14 (Conv2D)          (None, 512, 512, 16)         2320      ['add_27[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_14 (Ba  (None, 512, 512, 16)         64        ['conv2d_14[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " re_lu_14 (ReLU)             (None, 512, 512, 16)         0         ['batch_normalization_14[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_15 (Conv2D)          (None, 512, 512, 16)         2320      ['re_lu_14[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_15 (Ba  (None, 512, 512, 16)         64        ['conv2d_15[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " re_lu_15 (ReLU)             (None, 512, 512, 16)         0         ['batch_normalization_15[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_16 (Conv2D)          (None, 512, 512, 1)          17        ['re_lu_15[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2202353 (8.40 MB)\n",
      "Trainable params: 2200625 (8.39 MB)\n",
      "Non-trainable params: 1728 (6.75 KB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def multilp(x, o):\n",
    "    x = L.Dense(o[\"mp_dim\"], activation=\"gelu\")(x)\n",
    "    x = L.Dropout(o[\"dropout_rate\"])(x)\n",
    "    x = L.Dense(o[\"hidden_dim\"])(x)\n",
    "    x = L.Dropout(o[\"dropout_rate\"])(x)\n",
    "    return x\n",
    "\n",
    "def transformer_er(x, o):\n",
    "    sk_1 = x\n",
    "    x = L.LayerNormalization()(x)\n",
    "    x = L.MultiHeadAttention(\n",
    "        num_heads=o[\"n_heads\"], key_dim=o[\"hidden_dim\"]\n",
    "    )(x, x)\n",
    "    x = L.Add()([x, sk_1])\n",
    "\n",
    "    sk_2 = x\n",
    "    x = L.LayerNormalization()(x)\n",
    "    x = multilp(x, o)\n",
    "    x = L.Add()([x, sk_2])\n",
    "\n",
    "    return x\n",
    "\n",
    "def conv_bk(x, num_filters, kernel_size=3):\n",
    "    x = L.Conv2D(num_filters, kernel_size=kernel_size, padding=\"same\")(x)\n",
    "    x = L.BatchNormalization()(x)\n",
    "    x = L.ReLU()(x)\n",
    "    return x\n",
    "\n",
    "def deconv_bk(x, n_filters, std=2):\n",
    "    x = L.Conv2DTranspose(n_filters, kernel_size=2, padding=\"same\", strides=std)(x)\n",
    "    return x\n",
    "\n",
    "def build_unenr_2d(o):\n",
    "    \"\"\" Inputs \"\"\"\n",
    "    input_s = (o[\"n_patches\"], o[\"patch_s\"]*o[\"patch_s\"]*o[\"n_channels\"])\n",
    "    inp = L.Input(input_s) ## (None, 256, 3072)\n",
    "\n",
    "    \"\"\" Patch + Position Embeddings \"\"\"\n",
    "    patch_e = L.Dense(o[\"hidden_dim\"])(inp) ## (None, 256, 768)\n",
    "\n",
    "    ps = tf.range(start=0, limit=o[\"n_patches\"], delta=1) ## (256,)\n",
    "    pos_e = L.Embedding(input_dim=o[\"n_patches\"], output_dim=o[\"hidden_dim\"])(ps) ## (256, 768)\n",
    "    x = patch_e + pos_e ## (None, 256, 768)\n",
    "\n",
    "    \"\"\" Transformer Encoder \"\"\"\n",
    "    sk_connection_i = [3, 6, 9, 12]\n",
    "    sk_connection = []\n",
    "\n",
    "    for i in range(1, o[\"n_layers\"]+1, 1):\n",
    "        x = transformer_er(x, o)\n",
    "\n",
    "        if i in sk_connection_i:\n",
    "            sk_connection.append(x)\n",
    "\n",
    "    \"\"\" CNN Decoder \"\"\"\n",
    "    s3, s6, s9, s12 = sk_connection\n",
    "\n",
    "    ## Reshaping\n",
    "    s0 = L.Reshape((o[\"i_size\"], o[\"i_size\"], o[\"n_channels\"]))(inp)\n",
    "\n",
    "    shape = (\n",
    "        o[\"i_size\"]//o[\"patch_s\"],\n",
    "        o[\"i_size\"]//o[\"patch_s\"],\n",
    "        o[\"hidden_dim\"]\n",
    "    )\n",
    "    s3 = L.Reshape(shape)(s3)\n",
    "    s6 = L.Reshape(shape)(s6)\n",
    "    s9 = L.Reshape(shape)(s9)\n",
    "    s12 = L.Reshape(shape)(s12)\n",
    "\n",
    "    ## Additional layers for managing different patch sizes\n",
    "    total_addi_factor = int(log2(o[\"patch_s\"]))\n",
    "    addi = total_addi_factor - 4\n",
    "\n",
    "    if addi >= 2: ## Patch size 16 or greater\n",
    "        s3 = deconv_bk(s3, s3.shape[-1], strides=2**addi)\n",
    "        s6 = deconv_bk(s6, s6.shape[-1], strides=2**addi)\n",
    "        s9 = deconv_bk(s9, s9.shape[-1], strides=2**addi)\n",
    "        s12 = deconv_bk(s12, s12.shape[-1], strides=2**addi)\n",
    "        # print(z3.shape, z6.shape, z9.shape, z12.shape)\n",
    "\n",
    "    if addi < 0: ## Patch size less than 16\n",
    "        p = 2**abs(addi)\n",
    "        s3 = L.MaxPool2D((p, p))(s3)\n",
    "        s6 = L.MaxPool2D((p, p))(s6)\n",
    "        s9 = L.MaxPool2D((p, p))(s9)\n",
    "        s12 = L.MaxPool2D((p, p))(s12)\n",
    "\n",
    "    ## Decoder 1\n",
    "    x = deconv_bk(s12, 128)\n",
    "\n",
    "    s = deconv_bk(s9, 128)\n",
    "    s = conv_bk(s, 128)\n",
    "\n",
    "    x = L.Add()([0.7 * x, 0.3 * s])#L.Concatenate()([x, s])\n",
    "\n",
    "    x = conv_bk(x, 128)\n",
    "    x = conv_bk(x, 128)\n",
    "\n",
    "    ## Decoder 2\n",
    "    x = deconv_bk(x, 64)\n",
    "\n",
    "    s = deconv_bk(s6, 64)\n",
    "    s = conv_bk(s, 64)\n",
    "    s = deconv_bk(s, 64)\n",
    "    s = conv_bk(s, 64)\n",
    "\n",
    "    x = L.Add()([0.7 * x, 0.3 * s])#L.Concatenate()([x, s])\n",
    "    x = conv_bk(x, 64)\n",
    "    x = conv_bk(x, 64)\n",
    "\n",
    "    ## Decoder 3\n",
    "    x = deconv_bk(x, 32)\n",
    "\n",
    "    s = deconv_bk(s3, 32)\n",
    "    s = conv_bk(s, 32)\n",
    "    s = deconv_bk(s, 32)\n",
    "    s = conv_bk(s, 32)\n",
    "    s = deconv_bk(s, 32)\n",
    "    s = conv_bk(s, 32)\n",
    "\n",
    "    x = L.Add()([0.7 * x, 0.3 * s])#L.Concatenate()([x, s])\n",
    "    x = conv_bk(x, 32)\n",
    "    x = conv_bk(x, 32)\n",
    "\n",
    "    ## Decoder 4\n",
    "    x = deconv_bk(x, 16)\n",
    "\n",
    "    s = conv_bk(s0, 16)\n",
    "    s = conv_bk(s, 16)\n",
    "\n",
    "    x = L.Add()([0.7 * x, 0.3 * s])#L.Concatenate()([x, s])\n",
    "    x = conv_bk(x, 16)\n",
    "    x = conv_bk(x, 16)\n",
    "\n",
    "    \"\"\" Output \"\"\"\n",
    "    outputs = L.Conv2D(1, kernel_size=1, padding=\"same\", activation=\"sigmoid\")(x)\n",
    "\n",
    "    return Model(inp, outputs, name=\"UNETR_2D\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    config = {}\n",
    "    config[\"i_size\"] = 512\n",
    "    config[\"n_layers\"] = 12\n",
    "    config[\"hidden_dim\"] = 64\n",
    "    config[\"mp_dim\"] = 128\n",
    "    config[\"n_heads\"] = 6\n",
    "    config[\"dropout_rate\"] = 0.1\n",
    "    config[\"patch_s\"] = 1\n",
    "    config[\"n_patches\"] = (config[\"i_size\"]**2)//(config[\"patch_s\"]**2)\n",
    "    config[\"n_channels\"] = 3\n",
    "\n",
    "    model = build_unenr_2d(config)\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a8d5d2a-5848-472c-98fa-aafd22ee0236",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: \t256 - 256\n",
      "Valid: \t31 - 31\n",
      "Test: \t31 - 31\n",
      "Epoch 1/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.9850 - d_coef: 0.0150 - accuracy: 0.7053\n",
      "Epoch 1: val_loss improved from inf to 0.98492, saving model to files\\model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GOWTHAM\\anaconda3\\envs\\Gowtham\\lib\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 77s 2s/step - loss: 0.9850 - d_coef: 0.0150 - accuracy: 0.7053 - val_loss: 0.9849 - val_d_coef: 0.0154 - val_accuracy: 0.9892 - lr: 0.1000\n",
      "Epoch 2/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.9843 - d_coef: 0.0157 - accuracy: 0.7053\n",
      "Epoch 2: val_loss did not improve from 0.98492\n",
      "32/32 [==============================] - 61s 2s/step - loss: 0.9843 - d_coef: 0.0157 - accuracy: 0.7053 - val_loss: 0.9849 - val_d_coef: 0.0154 - val_accuracy: 0.9892 - lr: 0.1000\n",
      "Epoch 3/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.9834 - d_coef: 0.0166 - accuracy: 0.7032\n",
      "Epoch 3: val_loss did not improve from 0.98492\n",
      "32/32 [==============================] - 64s 2s/step - loss: 0.9834 - d_coef: 0.0166 - accuracy: 0.7032 - val_loss: 0.9849 - val_d_coef: 0.0154 - val_accuracy: 0.9892 - lr: 0.1000\n",
      "Epoch 4/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.9821 - d_coef: 0.0179 - accuracy: 0.7002\n",
      "Epoch 4: val_loss improved from 0.98492 to 0.98485, saving model to files\\model.h5\n",
      "32/32 [==============================] - 69s 2s/step - loss: 0.9821 - d_coef: 0.0179 - accuracy: 0.7002 - val_loss: 0.9848 - val_d_coef: 0.0155 - val_accuracy: 0.9892 - lr: 0.1000\n",
      "Epoch 5/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.9800 - d_coef: 0.0200 - accuracy: 0.6969\n",
      "Epoch 5: val_loss improved from 0.98485 to 0.98446, saving model to files\\model.h5\n",
      "32/32 [==============================] - 62s 2s/step - loss: 0.9800 - d_coef: 0.0200 - accuracy: 0.6969 - val_loss: 0.9845 - val_d_coef: 0.0159 - val_accuracy: 0.9892 - lr: 0.1000\n",
      "Epoch 6/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.9774 - d_coef: 0.0226 - accuracy: 0.6936\n",
      "Epoch 6: val_loss improved from 0.98446 to 0.98325, saving model to files\\model.h5\n",
      "32/32 [==============================] - 64s 2s/step - loss: 0.9774 - d_coef: 0.0226 - accuracy: 0.6936 - val_loss: 0.9833 - val_d_coef: 0.0171 - val_accuracy: 0.9890 - lr: 0.1000\n",
      "Epoch 7/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.9743 - d_coef: 0.0257 - accuracy: 0.6912\n",
      "Epoch 7: val_loss improved from 0.98325 to 0.97991, saving model to files\\model.h5\n",
      "32/32 [==============================] - 64s 2s/step - loss: 0.9743 - d_coef: 0.0257 - accuracy: 0.6912 - val_loss: 0.9799 - val_d_coef: 0.0206 - val_accuracy: 0.9763 - lr: 0.1000\n",
      "Epoch 8/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.9711 - d_coef: 0.0289 - accuracy: 0.6956\n",
      "Epoch 8: val_loss improved from 0.97991 to 0.97274, saving model to files\\model.h5\n",
      "32/32 [==============================] - 65s 2s/step - loss: 0.9711 - d_coef: 0.0289 - accuracy: 0.6956 - val_loss: 0.9727 - val_d_coef: 0.0279 - val_accuracy: 0.9120 - lr: 0.1000\n",
      "Epoch 9/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.9683 - d_coef: 0.0317 - accuracy: 0.7100\n",
      "Epoch 9: val_loss improved from 0.97274 to 0.96497, saving model to files\\model.h5\n",
      "32/32 [==============================] - 67s 2s/step - loss: 0.9683 - d_coef: 0.0317 - accuracy: 0.7100 - val_loss: 0.9650 - val_d_coef: 0.0359 - val_accuracy: 0.8527 - lr: 0.1000\n",
      "Epoch 10/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.9655 - d_coef: 0.0345 - accuracy: 0.7374\n",
      "Epoch 10: val_loss improved from 0.96497 to 0.95913, saving model to files\\model.h5\n",
      "32/32 [==============================] - 64s 2s/step - loss: 0.9655 - d_coef: 0.0345 - accuracy: 0.7374 - val_loss: 0.9591 - val_d_coef: 0.0419 - val_accuracy: 0.8335 - lr: 0.1000\n",
      "Epoch 11/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.9627 - d_coef: 0.0373 - accuracy: 0.7700\n",
      "Epoch 11: val_loss improved from 0.95913 to 0.95545, saving model to files\\model.h5\n",
      "32/32 [==============================] - 63s 2s/step - loss: 0.9627 - d_coef: 0.0373 - accuracy: 0.7700 - val_loss: 0.9554 - val_d_coef: 0.0456 - val_accuracy: 0.8175 - lr: 0.1000\n",
      "Epoch 12/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.9595 - d_coef: 0.0405 - accuracy: 0.7962\n",
      "Epoch 12: val_loss improved from 0.95545 to 0.95354, saving model to files\\model.h5\n",
      "32/32 [==============================] - 64s 2s/step - loss: 0.9595 - d_coef: 0.0405 - accuracy: 0.7962 - val_loss: 0.9535 - val_d_coef: 0.0475 - val_accuracy: 0.8016 - lr: 0.1000\n",
      "Epoch 13/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.9562 - d_coef: 0.0438 - accuracy: 0.8199\n",
      "Epoch 13: val_loss improved from 0.95354 to 0.95064, saving model to files\\model.h5\n",
      "32/32 [==============================] - 63s 2s/step - loss: 0.9562 - d_coef: 0.0438 - accuracy: 0.8199 - val_loss: 0.9506 - val_d_coef: 0.0505 - val_accuracy: 0.8060 - lr: 0.1000\n",
      "Epoch 14/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.9518 - d_coef: 0.0482 - accuracy: 0.8410\n",
      "Epoch 14: val_loss improved from 0.95064 to 0.94726, saving model to files\\model.h5\n",
      "32/32 [==============================] - 62s 2s/step - loss: 0.9518 - d_coef: 0.0482 - accuracy: 0.8410 - val_loss: 0.9473 - val_d_coef: 0.0539 - val_accuracy: 0.8149 - lr: 0.1000\n",
      "Epoch 15/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.9467 - d_coef: 0.0533 - accuracy: 0.8606\n",
      "Epoch 15: val_loss improved from 0.94726 to 0.94328, saving model to files\\model.h5\n",
      "32/32 [==============================] - 61s 2s/step - loss: 0.9467 - d_coef: 0.0533 - accuracy: 0.8606 - val_loss: 0.9433 - val_d_coef: 0.0580 - val_accuracy: 0.8239 - lr: 0.1000\n",
      "Epoch 16/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.9405 - d_coef: 0.0595 - accuracy: 0.8783\n",
      "Epoch 16: val_loss improved from 0.94328 to 0.93393, saving model to files\\model.h5\n",
      "32/32 [==============================] - 61s 2s/step - loss: 0.9405 - d_coef: 0.0595 - accuracy: 0.8783 - val_loss: 0.9339 - val_d_coef: 0.0675 - val_accuracy: 0.8499 - lr: 0.1000\n",
      "Epoch 17/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.9321 - d_coef: 0.0679 - accuracy: 0.8964\n",
      "Epoch 17: val_loss improved from 0.93393 to 0.91891, saving model to files\\model.h5\n",
      "32/32 [==============================] - 62s 2s/step - loss: 0.9321 - d_coef: 0.0679 - accuracy: 0.8964 - val_loss: 0.9189 - val_d_coef: 0.0829 - val_accuracy: 0.8805 - lr: 0.1000\n",
      "Epoch 18/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.9220 - d_coef: 0.0780 - accuracy: 0.9146\n",
      "Epoch 18: val_loss improved from 0.91891 to 0.90586, saving model to files\\model.h5\n",
      "32/32 [==============================] - 64s 2s/step - loss: 0.9220 - d_coef: 0.0780 - accuracy: 0.9146 - val_loss: 0.9059 - val_d_coef: 0.0962 - val_accuracy: 0.8929 - lr: 0.1000\n",
      "Epoch 19/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.9061 - d_coef: 0.0939 - accuracy: 0.9338\n",
      "Epoch 19: val_loss improved from 0.90586 to 0.87996, saving model to files\\model.h5\n",
      "32/32 [==============================] - 69s 2s/step - loss: 0.9061 - d_coef: 0.0939 - accuracy: 0.9338 - val_loss: 0.8800 - val_d_coef: 0.1228 - val_accuracy: 0.9165 - lr: 0.1000\n",
      "Epoch 20/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.8870 - d_coef: 0.1130 - accuracy: 0.9486\n",
      "Epoch 20: val_loss improved from 0.87996 to 0.84297, saving model to files\\model.h5\n",
      "32/32 [==============================] - 62s 2s/step - loss: 0.8870 - d_coef: 0.1130 - accuracy: 0.9486 - val_loss: 0.8430 - val_d_coef: 0.1607 - val_accuracy: 0.9431 - lr: 0.1000\n",
      "Epoch 21/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.8585 - d_coef: 0.1415 - accuracy: 0.9647\n",
      "Epoch 21: val_loss did not improve from 0.84297\n",
      "32/32 [==============================] - 64s 2s/step - loss: 0.8585 - d_coef: 0.1415 - accuracy: 0.9647 - val_loss: 0.9044 - val_d_coef: 0.0976 - val_accuracy: 0.8730 - lr: 0.1000\n",
      "Epoch 22/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.8267 - d_coef: 0.1733 - accuracy: 0.9707\n",
      "Epoch 22: val_loss did not improve from 0.84297\n",
      "32/32 [==============================] - 64s 2s/step - loss: 0.8267 - d_coef: 0.1733 - accuracy: 0.9707 - val_loss: 0.8573 - val_d_coef: 0.1462 - val_accuracy: 0.9270 - lr: 0.1000\n",
      "Epoch 23/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.8256 - d_coef: 0.1744 - accuracy: 0.9695\n",
      "Epoch 23: val_loss did not improve from 0.84297\n",
      "32/32 [==============================] - 62s 2s/step - loss: 0.8256 - d_coef: 0.1744 - accuracy: 0.9695 - val_loss: 0.9276 - val_d_coef: 0.0739 - val_accuracy: 0.8219 - lr: 0.1000\n",
      "Epoch 24/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.7857 - d_coef: 0.2143 - accuracy: 0.9775\n",
      "Epoch 24: val_loss improved from 0.84297 to 0.73057, saving model to files\\model.h5\n",
      "32/32 [==============================] - 63s 2s/step - loss: 0.7857 - d_coef: 0.2143 - accuracy: 0.9775 - val_loss: 0.7306 - val_d_coef: 0.2767 - val_accuracy: 0.9698 - lr: 0.1000\n",
      "Epoch 25/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.7664 - d_coef: 0.2336 - accuracy: 0.9797\n",
      "Epoch 25: val_loss improved from 0.73057 to 0.72883, saving model to files\\model.h5\n",
      "32/32 [==============================] - 62s 2s/step - loss: 0.7664 - d_coef: 0.2336 - accuracy: 0.9797 - val_loss: 0.7288 - val_d_coef: 0.2784 - val_accuracy: 0.9692 - lr: 0.1000\n",
      "Epoch 26/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.7564 - d_coef: 0.2436 - accuracy: 0.9796\n",
      "Epoch 26: val_loss did not improve from 0.72883\n",
      "32/32 [==============================] - 62s 2s/step - loss: 0.7564 - d_coef: 0.2436 - accuracy: 0.9796 - val_loss: 0.9545 - val_d_coef: 0.0464 - val_accuracy: 0.6917 - lr: 0.1000\n",
      "Epoch 27/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.7537 - d_coef: 0.2463 - accuracy: 0.9807\n",
      "Epoch 27: val_loss did not improve from 0.72883\n",
      "32/32 [==============================] - 61s 2s/step - loss: 0.7537 - d_coef: 0.2463 - accuracy: 0.9807 - val_loss: 0.7672 - val_d_coef: 0.2387 - val_accuracy: 0.9617 - lr: 0.1000\n",
      "Epoch 28/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.7299 - d_coef: 0.2701 - accuracy: 0.9834\n",
      "Epoch 28: val_loss improved from 0.72883 to 0.61249, saving model to files\\model.h5\n",
      "32/32 [==============================] - 62s 2s/step - loss: 0.7299 - d_coef: 0.2701 - accuracy: 0.9834 - val_loss: 0.6125 - val_d_coef: 0.3964 - val_accuracy: 0.9845 - lr: 0.1000\n",
      "Epoch 29/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.7201 - d_coef: 0.2799 - accuracy: 0.9819\n",
      "Epoch 29: val_loss improved from 0.61249 to 0.58041, saving model to files\\model.h5\n",
      "32/32 [==============================] - 62s 2s/step - loss: 0.7201 - d_coef: 0.2799 - accuracy: 0.9819 - val_loss: 0.5804 - val_d_coef: 0.4269 - val_accuracy: 0.9888 - lr: 0.1000\n",
      "Epoch 30/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.7079 - d_coef: 0.2921 - accuracy: 0.9846\n",
      "Epoch 30: val_loss did not improve from 0.58041\n",
      "32/32 [==============================] - 65s 2s/step - loss: 0.7079 - d_coef: 0.2921 - accuracy: 0.9846 - val_loss: 0.5865 - val_d_coef: 0.4234 - val_accuracy: 0.9866 - lr: 0.1000\n",
      "Epoch 31/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.6957 - d_coef: 0.3043 - accuracy: 0.9841\n",
      "Epoch 31: val_loss did not improve from 0.58041\n",
      "32/32 [==============================] - 62s 2s/step - loss: 0.6957 - d_coef: 0.3043 - accuracy: 0.9841 - val_loss: 0.7192 - val_d_coef: 0.2862 - val_accuracy: 0.9900 - lr: 0.1000\n",
      "Epoch 32/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.6775 - d_coef: 0.3225 - accuracy: 0.9862\n",
      "Epoch 32: val_loss did not improve from 0.58041\n",
      "32/32 [==============================] - 63s 2s/step - loss: 0.6775 - d_coef: 0.3225 - accuracy: 0.9862 - val_loss: 0.8338 - val_d_coef: 0.1702 - val_accuracy: 0.9389 - lr: 0.1000\n",
      "Epoch 33/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.6510 - d_coef: 0.3490 - accuracy: 0.9869\n",
      "Epoch 33: val_loss improved from 0.58041 to 0.53519, saving model to files\\model.h5\n",
      "32/32 [==============================] - 66s 2s/step - loss: 0.6510 - d_coef: 0.3490 - accuracy: 0.9869 - val_loss: 0.5352 - val_d_coef: 0.4700 - val_accuracy: 0.9904 - lr: 0.1000\n",
      "Epoch 34/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.6945 - d_coef: 0.3055 - accuracy: 0.9853\n",
      "Epoch 34: val_loss did not improve from 0.53519\n",
      "32/32 [==============================] - 63s 2s/step - loss: 0.6945 - d_coef: 0.3055 - accuracy: 0.9853 - val_loss: 0.7091 - val_d_coef: 0.2970 - val_accuracy: 0.9716 - lr: 0.1000\n",
      "Epoch 35/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.6344 - d_coef: 0.3656 - accuracy: 0.9874\n",
      "Epoch 35: val_loss improved from 0.53519 to 0.53018, saving model to files\\model.h5\n",
      "32/32 [==============================] - 63s 2s/step - loss: 0.6344 - d_coef: 0.3656 - accuracy: 0.9874 - val_loss: 0.5302 - val_d_coef: 0.4758 - val_accuracy: 0.9888 - lr: 0.1000\n",
      "Epoch 36/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.6307 - d_coef: 0.3693 - accuracy: 0.9880\n",
      "Epoch 36: val_loss did not improve from 0.53018\n",
      "32/32 [==============================] - 62s 2s/step - loss: 0.6307 - d_coef: 0.3693 - accuracy: 0.9880 - val_loss: 0.9958 - val_d_coef: 0.0046 - val_accuracy: 0.9892 - lr: 0.1000\n",
      "Epoch 37/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.6205 - d_coef: 0.3795 - accuracy: 0.9881\n",
      "Epoch 37: val_loss did not improve from 0.53018\n",
      "32/32 [==============================] - 61s 2s/step - loss: 0.6205 - d_coef: 0.3795 - accuracy: 0.9881 - val_loss: 0.6850 - val_d_coef: 0.3218 - val_accuracy: 0.9908 - lr: 0.1000\n",
      "Epoch 38/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.6007 - d_coef: 0.3993 - accuracy: 0.9888\n",
      "Epoch 38: val_loss did not improve from 0.53018\n",
      "32/32 [==============================] - 62s 2s/step - loss: 0.6007 - d_coef: 0.3993 - accuracy: 0.9888 - val_loss: 0.6806 - val_d_coef: 0.3258 - val_accuracy: 0.9772 - lr: 0.1000\n",
      "Epoch 39/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.6317 - d_coef: 0.3683 - accuracy: 0.9866\n",
      "Epoch 39: val_loss did not improve from 0.53018\n",
      "32/32 [==============================] - 63s 2s/step - loss: 0.6317 - d_coef: 0.3683 - accuracy: 0.9866 - val_loss: 0.7471 - val_d_coef: 0.2604 - val_accuracy: 0.9643 - lr: 0.1000\n",
      "Epoch 40/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.6054 - d_coef: 0.3946 - accuracy: 0.9880\n",
      "Epoch 40: val_loss did not improve from 0.53018\n",
      "\n",
      "Epoch 40: ReduceLROnPlateau reducing learning rate to 0.010000000149011612.\n",
      "32/32 [==============================] - 62s 2s/step - loss: 0.6054 - d_coef: 0.3946 - accuracy: 0.9880 - val_loss: 0.6922 - val_d_coef: 0.3184 - val_accuracy: 0.9908 - lr: 0.1000\n",
      "Epoch 41/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.5924 - d_coef: 0.4076 - accuracy: 0.9881\n",
      "Epoch 41: val_loss improved from 0.53018 to 0.46691, saving model to files\\model.h5\n",
      "32/32 [==============================] - 62s 2s/step - loss: 0.5924 - d_coef: 0.4076 - accuracy: 0.9881 - val_loss: 0.4669 - val_d_coef: 0.5398 - val_accuracy: 0.9916 - lr: 0.0100\n",
      "Epoch 42/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.5489 - d_coef: 0.4511 - accuracy: 0.9896\n",
      "Epoch 42: val_loss did not improve from 0.46691\n",
      "32/32 [==============================] - 62s 2s/step - loss: 0.5489 - d_coef: 0.4511 - accuracy: 0.9896 - val_loss: 0.6378 - val_d_coef: 0.3696 - val_accuracy: 0.9911 - lr: 0.0100\n",
      "Epoch 43/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.5345 - d_coef: 0.4655 - accuracy: 0.9902\n",
      "Epoch 43: val_loss did not improve from 0.46691\n",
      "32/32 [==============================] - 62s 2s/step - loss: 0.5345 - d_coef: 0.4655 - accuracy: 0.9902 - val_loss: 0.8339 - val_d_coef: 0.1732 - val_accuracy: 0.9901 - lr: 0.0100\n",
      "Epoch 44/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.5197 - d_coef: 0.4803 - accuracy: 0.9904\n",
      "Epoch 44: val_loss did not improve from 0.46691\n",
      "32/32 [==============================] - 63s 2s/step - loss: 0.5197 - d_coef: 0.4803 - accuracy: 0.9904 - val_loss: 0.6049 - val_d_coef: 0.4009 - val_accuracy: 0.9911 - lr: 0.0100\n",
      "Epoch 45/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.5097 - d_coef: 0.4903 - accuracy: 0.9906\n",
      "Epoch 45: val_loss did not improve from 0.46691\n",
      "32/32 [==============================] - 62s 2s/step - loss: 0.5097 - d_coef: 0.4903 - accuracy: 0.9906 - val_loss: 0.5849 - val_d_coef: 0.4213 - val_accuracy: 0.9913 - lr: 0.0100\n",
      "Epoch 46/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.5073 - d_coef: 0.4927 - accuracy: 0.9906\n",
      "Epoch 46: val_loss did not improve from 0.46691\n",
      "\n",
      "Epoch 46: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "32/32 [==============================] - 62s 2s/step - loss: 0.5073 - d_coef: 0.4927 - accuracy: 0.9906 - val_loss: 0.4942 - val_d_coef: 0.5110 - val_accuracy: 0.9917 - lr: 0.0100\n",
      "Epoch 47/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4944 - d_coef: 0.5056 - accuracy: 0.9907\n",
      "Epoch 47: val_loss improved from 0.46691 to 0.45793, saving model to files\\model.h5\n",
      "32/32 [==============================] - 62s 2s/step - loss: 0.4944 - d_coef: 0.5056 - accuracy: 0.9907 - val_loss: 0.4579 - val_d_coef: 0.5471 - val_accuracy: 0.9919 - lr: 1.0000e-03\n",
      "Epoch 48/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4987 - d_coef: 0.5013 - accuracy: 0.9908\n",
      "Epoch 48: val_loss improved from 0.45793 to 0.42379, saving model to files\\model.h5\n",
      "32/32 [==============================] - 63s 2s/step - loss: 0.4987 - d_coef: 0.5013 - accuracy: 0.9908 - val_loss: 0.4238 - val_d_coef: 0.5811 - val_accuracy: 0.9919 - lr: 1.0000e-03\n",
      "Epoch 49/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4952 - d_coef: 0.5048 - accuracy: 0.9908\n",
      "Epoch 49: val_loss improved from 0.42379 to 0.40592, saving model to files\\model.h5\n",
      "32/32 [==============================] - 62s 2s/step - loss: 0.4952 - d_coef: 0.5048 - accuracy: 0.9908 - val_loss: 0.4059 - val_d_coef: 0.5989 - val_accuracy: 0.9919 - lr: 1.0000e-03\n",
      "Epoch 50/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4926 - d_coef: 0.5074 - accuracy: 0.9908\n",
      "Epoch 50: val_loss improved from 0.40592 to 0.39743, saving model to files\\model.h5\n",
      "32/32 [==============================] - 62s 2s/step - loss: 0.4926 - d_coef: 0.5074 - accuracy: 0.9908 - val_loss: 0.3974 - val_d_coef: 0.6073 - val_accuracy: 0.9918 - lr: 1.0000e-03\n",
      "Epoch 51/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4925 - d_coef: 0.5075 - accuracy: 0.9908\n",
      "Epoch 51: val_loss improved from 0.39743 to 0.39403, saving model to files\\model.h5\n",
      "32/32 [==============================] - 64s 2s/step - loss: 0.4925 - d_coef: 0.5075 - accuracy: 0.9908 - val_loss: 0.3940 - val_d_coef: 0.6107 - val_accuracy: 0.9918 - lr: 1.0000e-03\n",
      "Epoch 52/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4865 - d_coef: 0.5135 - accuracy: 0.9910\n",
      "Epoch 52: val_loss improved from 0.39403 to 0.39231, saving model to files\\model.h5\n",
      "32/32 [==============================] - 66s 2s/step - loss: 0.4865 - d_coef: 0.5135 - accuracy: 0.9910 - val_loss: 0.3923 - val_d_coef: 0.6123 - val_accuracy: 0.9918 - lr: 1.0000e-03\n",
      "Epoch 53/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4890 - d_coef: 0.5110 - accuracy: 0.9909\n",
      "Epoch 53: val_loss improved from 0.39231 to 0.38955, saving model to files\\model.h5\n",
      "32/32 [==============================] - 69s 2s/step - loss: 0.4890 - d_coef: 0.5110 - accuracy: 0.9909 - val_loss: 0.3896 - val_d_coef: 0.6150 - val_accuracy: 0.9917 - lr: 1.0000e-03\n",
      "Epoch 54/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4939 - d_coef: 0.5061 - accuracy: 0.9908\n",
      "Epoch 54: val_loss did not improve from 0.38955\n",
      "32/32 [==============================] - 66s 2s/step - loss: 0.4939 - d_coef: 0.5061 - accuracy: 0.9908 - val_loss: 0.3899 - val_d_coef: 0.6147 - val_accuracy: 0.9915 - lr: 1.0000e-03\n",
      "Epoch 55/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4877 - d_coef: 0.5123 - accuracy: 0.9909\n",
      "Epoch 55: val_loss improved from 0.38955 to 0.38847, saving model to files\\model.h5\n",
      "32/32 [==============================] - 67s 2s/step - loss: 0.4877 - d_coef: 0.5123 - accuracy: 0.9909 - val_loss: 0.3885 - val_d_coef: 0.6161 - val_accuracy: 0.9916 - lr: 1.0000e-03\n",
      "Epoch 56/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4900 - d_coef: 0.5100 - accuracy: 0.9909\n",
      "Epoch 56: val_loss improved from 0.38847 to 0.38714, saving model to files\\model.h5\n",
      "32/32 [==============================] - 66s 2s/step - loss: 0.4900 - d_coef: 0.5100 - accuracy: 0.9909 - val_loss: 0.3871 - val_d_coef: 0.6174 - val_accuracy: 0.9916 - lr: 1.0000e-03\n",
      "Epoch 57/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4826 - d_coef: 0.5174 - accuracy: 0.9910\n",
      "Epoch 57: val_loss improved from 0.38714 to 0.38605, saving model to files\\model.h5\n",
      "32/32 [==============================] - 65s 2s/step - loss: 0.4826 - d_coef: 0.5174 - accuracy: 0.9910 - val_loss: 0.3860 - val_d_coef: 0.6185 - val_accuracy: 0.9917 - lr: 1.0000e-03\n",
      "Epoch 58/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4818 - d_coef: 0.5182 - accuracy: 0.9910\n",
      "Epoch 58: val_loss improved from 0.38605 to 0.38468, saving model to files\\model.h5\n",
      "32/32 [==============================] - 64s 2s/step - loss: 0.4818 - d_coef: 0.5182 - accuracy: 0.9910 - val_loss: 0.3847 - val_d_coef: 0.6198 - val_accuracy: 0.9917 - lr: 1.0000e-03\n",
      "Epoch 59/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4815 - d_coef: 0.5185 - accuracy: 0.9910\n",
      "Epoch 59: val_loss did not improve from 0.38468\n",
      "32/32 [==============================] - 64s 2s/step - loss: 0.4815 - d_coef: 0.5185 - accuracy: 0.9910 - val_loss: 0.3854 - val_d_coef: 0.6192 - val_accuracy: 0.9916 - lr: 1.0000e-03\n",
      "Epoch 60/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4836 - d_coef: 0.5164 - accuracy: 0.9910\n",
      "Epoch 60: val_loss improved from 0.38468 to 0.38427, saving model to files\\model.h5\n",
      "32/32 [==============================] - 64s 2s/step - loss: 0.4836 - d_coef: 0.5164 - accuracy: 0.9910 - val_loss: 0.3843 - val_d_coef: 0.6202 - val_accuracy: 0.9917 - lr: 1.0000e-03\n",
      "Epoch 61/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4845 - d_coef: 0.5155 - accuracy: 0.9910\n",
      "Epoch 61: val_loss improved from 0.38427 to 0.38361, saving model to files\\model.h5\n",
      "32/32 [==============================] - 64s 2s/step - loss: 0.4845 - d_coef: 0.5155 - accuracy: 0.9910 - val_loss: 0.3836 - val_d_coef: 0.6209 - val_accuracy: 0.9917 - lr: 1.0000e-03\n",
      "Epoch 62/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4842 - d_coef: 0.5158 - accuracy: 0.9910\n",
      "Epoch 62: val_loss improved from 0.38361 to 0.38176, saving model to files\\model.h5\n",
      "32/32 [==============================] - 63s 2s/step - loss: 0.4842 - d_coef: 0.5158 - accuracy: 0.9910 - val_loss: 0.3818 - val_d_coef: 0.6227 - val_accuracy: 0.9917 - lr: 1.0000e-03\n",
      "Epoch 63/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4850 - d_coef: 0.5150 - accuracy: 0.9910\n",
      "Epoch 63: val_loss improved from 0.38176 to 0.38117, saving model to files\\model.h5\n",
      "32/32 [==============================] - 63s 2s/step - loss: 0.4850 - d_coef: 0.5150 - accuracy: 0.9910 - val_loss: 0.3812 - val_d_coef: 0.6233 - val_accuracy: 0.9917 - lr: 1.0000e-03\n",
      "Epoch 64/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4804 - d_coef: 0.5196 - accuracy: 0.9911\n",
      "Epoch 64: val_loss did not improve from 0.38117\n",
      "32/32 [==============================] - 61s 2s/step - loss: 0.4804 - d_coef: 0.5196 - accuracy: 0.9911 - val_loss: 0.3814 - val_d_coef: 0.6231 - val_accuracy: 0.9917 - lr: 1.0000e-03\n",
      "Epoch 65/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4838 - d_coef: 0.5162 - accuracy: 0.9910\n",
      "Epoch 65: val_loss did not improve from 0.38117\n",
      "32/32 [==============================] - 62s 2s/step - loss: 0.4838 - d_coef: 0.5162 - accuracy: 0.9910 - val_loss: 0.3820 - val_d_coef: 0.6225 - val_accuracy: 0.9917 - lr: 1.0000e-03\n",
      "Epoch 66/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4825 - d_coef: 0.5175 - accuracy: 0.9910\n",
      "Epoch 66: val_loss improved from 0.38117 to 0.38107, saving model to files\\model.h5\n",
      "32/32 [==============================] - 62s 2s/step - loss: 0.4825 - d_coef: 0.5175 - accuracy: 0.9910 - val_loss: 0.3811 - val_d_coef: 0.6234 - val_accuracy: 0.9917 - lr: 1.0000e-03\n",
      "Epoch 67/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4838 - d_coef: 0.5162 - accuracy: 0.9910\n",
      "Epoch 67: val_loss improved from 0.38107 to 0.38048, saving model to files\\model.h5\n",
      "32/32 [==============================] - 63s 2s/step - loss: 0.4838 - d_coef: 0.5162 - accuracy: 0.9910 - val_loss: 0.3805 - val_d_coef: 0.6239 - val_accuracy: 0.9916 - lr: 1.0000e-03\n",
      "Epoch 68/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4789 - d_coef: 0.5211 - accuracy: 0.9910\n",
      "Epoch 68: val_loss improved from 0.38048 to 0.37904, saving model to files\\model.h5\n",
      "32/32 [==============================] - 62s 2s/step - loss: 0.4789 - d_coef: 0.5211 - accuracy: 0.9910 - val_loss: 0.3790 - val_d_coef: 0.6254 - val_accuracy: 0.9918 - lr: 1.0000e-03\n",
      "Epoch 69/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4827 - d_coef: 0.5173 - accuracy: 0.9911\n",
      "Epoch 69: val_loss improved from 0.37904 to 0.37852, saving model to files\\model.h5\n",
      "32/32 [==============================] - 62s 2s/step - loss: 0.4827 - d_coef: 0.5173 - accuracy: 0.9911 - val_loss: 0.3785 - val_d_coef: 0.6259 - val_accuracy: 0.9918 - lr: 1.0000e-03\n",
      "Epoch 70/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4748 - d_coef: 0.5252 - accuracy: 0.9911\n",
      "Epoch 70: val_loss improved from 0.37852 to 0.37781, saving model to files\\model.h5\n",
      "32/32 [==============================] - 62s 2s/step - loss: 0.4748 - d_coef: 0.5252 - accuracy: 0.9911 - val_loss: 0.3778 - val_d_coef: 0.6266 - val_accuracy: 0.9918 - lr: 1.0000e-03\n",
      "Epoch 71/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4727 - d_coef: 0.5273 - accuracy: 0.9912\n",
      "Epoch 71: val_loss improved from 0.37781 to 0.37762, saving model to files\\model.h5\n",
      "32/32 [==============================] - 62s 2s/step - loss: 0.4727 - d_coef: 0.5273 - accuracy: 0.9912 - val_loss: 0.3776 - val_d_coef: 0.6268 - val_accuracy: 0.9917 - lr: 1.0000e-03\n",
      "Epoch 72/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4870 - d_coef: 0.5130 - accuracy: 0.9910\n",
      "Epoch 72: val_loss did not improve from 0.37762\n",
      "32/32 [==============================] - 62s 2s/step - loss: 0.4870 - d_coef: 0.5130 - accuracy: 0.9910 - val_loss: 0.3826 - val_d_coef: 0.6219 - val_accuracy: 0.9915 - lr: 1.0000e-03\n",
      "Epoch 73/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4773 - d_coef: 0.5227 - accuracy: 0.9911\n",
      "Epoch 73: val_loss improved from 0.37762 to 0.37761, saving model to files\\model.h5\n",
      "32/32 [==============================] - 61s 2s/step - loss: 0.4773 - d_coef: 0.5227 - accuracy: 0.9911 - val_loss: 0.3776 - val_d_coef: 0.6268 - val_accuracy: 0.9917 - lr: 1.0000e-03\n",
      "Epoch 74/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4796 - d_coef: 0.5204 - accuracy: 0.9911\n",
      "Epoch 74: val_loss improved from 0.37761 to 0.37748, saving model to files\\model.h5\n",
      "32/32 [==============================] - 62s 2s/step - loss: 0.4796 - d_coef: 0.5204 - accuracy: 0.9911 - val_loss: 0.3775 - val_d_coef: 0.6269 - val_accuracy: 0.9918 - lr: 1.0000e-03\n",
      "Epoch 75/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4729 - d_coef: 0.5271 - accuracy: 0.9912\n",
      "Epoch 75: val_loss improved from 0.37748 to 0.37651, saving model to files\\model.h5\n",
      "32/32 [==============================] - 62s 2s/step - loss: 0.4729 - d_coef: 0.5271 - accuracy: 0.9912 - val_loss: 0.3765 - val_d_coef: 0.6279 - val_accuracy: 0.9918 - lr: 1.0000e-03\n",
      "Epoch 76/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4780 - d_coef: 0.5220 - accuracy: 0.9911\n",
      "Epoch 76: val_loss improved from 0.37651 to 0.37572, saving model to files\\model.h5\n",
      "32/32 [==============================] - 63s 2s/step - loss: 0.4780 - d_coef: 0.5220 - accuracy: 0.9911 - val_loss: 0.3757 - val_d_coef: 0.6287 - val_accuracy: 0.9918 - lr: 1.0000e-03\n",
      "Epoch 77/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4805 - d_coef: 0.5195 - accuracy: 0.9910\n",
      "Epoch 77: val_loss did not improve from 0.37572\n",
      "32/32 [==============================] - 61s 2s/step - loss: 0.4805 - d_coef: 0.5195 - accuracy: 0.9910 - val_loss: 0.3760 - val_d_coef: 0.6284 - val_accuracy: 0.9917 - lr: 1.0000e-03\n",
      "Epoch 78/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4742 - d_coef: 0.5258 - accuracy: 0.9911\n",
      "Epoch 78: val_loss improved from 0.37572 to 0.37499, saving model to files\\model.h5\n",
      "32/32 [==============================] - 62s 2s/step - loss: 0.4742 - d_coef: 0.5258 - accuracy: 0.9911 - val_loss: 0.3750 - val_d_coef: 0.6294 - val_accuracy: 0.9918 - lr: 1.0000e-03\n",
      "Epoch 79/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4778 - d_coef: 0.5222 - accuracy: 0.9911\n",
      "Epoch 79: val_loss did not improve from 0.37499\n",
      "32/32 [==============================] - 62s 2s/step - loss: 0.4778 - d_coef: 0.5222 - accuracy: 0.9911 - val_loss: 0.3758 - val_d_coef: 0.6286 - val_accuracy: 0.9918 - lr: 1.0000e-03\n",
      "Epoch 80/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4758 - d_coef: 0.5242 - accuracy: 0.9912\n",
      "Epoch 80: val_loss did not improve from 0.37499\n",
      "32/32 [==============================] - 62s 2s/step - loss: 0.4758 - d_coef: 0.5242 - accuracy: 0.9912 - val_loss: 0.3757 - val_d_coef: 0.6287 - val_accuracy: 0.9917 - lr: 1.0000e-03\n",
      "Epoch 81/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4727 - d_coef: 0.5273 - accuracy: 0.9911\n",
      "Epoch 81: val_loss did not improve from 0.37499\n",
      "32/32 [==============================] - 61s 2s/step - loss: 0.4727 - d_coef: 0.5273 - accuracy: 0.9911 - val_loss: 0.3770 - val_d_coef: 0.6274 - val_accuracy: 0.9916 - lr: 1.0000e-03\n",
      "Epoch 82/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4723 - d_coef: 0.5277 - accuracy: 0.9912\n",
      "Epoch 82: val_loss improved from 0.37499 to 0.37407, saving model to files\\model.h5\n",
      "32/32 [==============================] - 62s 2s/step - loss: 0.4723 - d_coef: 0.5277 - accuracy: 0.9912 - val_loss: 0.3741 - val_d_coef: 0.6303 - val_accuracy: 0.9919 - lr: 1.0000e-03\n",
      "Epoch 83/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4724 - d_coef: 0.5276 - accuracy: 0.9912\n",
      "Epoch 83: val_loss improved from 0.37407 to 0.37303, saving model to files\\model.h5\n",
      "32/32 [==============================] - 62s 2s/step - loss: 0.4724 - d_coef: 0.5276 - accuracy: 0.9912 - val_loss: 0.3730 - val_d_coef: 0.6313 - val_accuracy: 0.9918 - lr: 1.0000e-03\n",
      "Epoch 84/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4731 - d_coef: 0.5269 - accuracy: 0.9912\n",
      "Epoch 84: val_loss did not improve from 0.37303\n",
      "32/32 [==============================] - 62s 2s/step - loss: 0.4731 - d_coef: 0.5269 - accuracy: 0.9912 - val_loss: 0.3734 - val_d_coef: 0.6309 - val_accuracy: 0.9917 - lr: 1.0000e-03\n",
      "Epoch 85/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4702 - d_coef: 0.5298 - accuracy: 0.9912\n",
      "Epoch 85: val_loss improved from 0.37303 to 0.37262, saving model to files\\model.h5\n",
      "32/32 [==============================] - 62s 2s/step - loss: 0.4702 - d_coef: 0.5298 - accuracy: 0.9912 - val_loss: 0.3726 - val_d_coef: 0.6317 - val_accuracy: 0.9918 - lr: 1.0000e-03\n",
      "Epoch 86/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4697 - d_coef: 0.5303 - accuracy: 0.9912\n",
      "Epoch 86: val_loss did not improve from 0.37262\n",
      "32/32 [==============================] - 61s 2s/step - loss: 0.4697 - d_coef: 0.5303 - accuracy: 0.9912 - val_loss: 0.3746 - val_d_coef: 0.6298 - val_accuracy: 0.9917 - lr: 1.0000e-03\n",
      "Epoch 87/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4673 - d_coef: 0.5327 - accuracy: 0.9912\n",
      "Epoch 87: val_loss did not improve from 0.37262\n",
      "32/32 [==============================] - 62s 2s/step - loss: 0.4673 - d_coef: 0.5327 - accuracy: 0.9912 - val_loss: 0.3744 - val_d_coef: 0.6300 - val_accuracy: 0.9917 - lr: 1.0000e-03\n",
      "Epoch 88/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4731 - d_coef: 0.5269 - accuracy: 0.9912\n",
      "Epoch 88: val_loss did not improve from 0.37262\n",
      "32/32 [==============================] - 62s 2s/step - loss: 0.4731 - d_coef: 0.5269 - accuracy: 0.9912 - val_loss: 0.3738 - val_d_coef: 0.6306 - val_accuracy: 0.9917 - lr: 1.0000e-03\n",
      "Epoch 89/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4685 - d_coef: 0.5315 - accuracy: 0.9912\n",
      "Epoch 89: val_loss improved from 0.37262 to 0.37153, saving model to files\\model.h5\n",
      "32/32 [==============================] - 62s 2s/step - loss: 0.4685 - d_coef: 0.5315 - accuracy: 0.9912 - val_loss: 0.3715 - val_d_coef: 0.6328 - val_accuracy: 0.9918 - lr: 1.0000e-03\n",
      "Epoch 90/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4691 - d_coef: 0.5309 - accuracy: 0.9912\n",
      "Epoch 90: val_loss did not improve from 0.37153\n",
      "32/32 [==============================] - 61s 2s/step - loss: 0.4691 - d_coef: 0.5309 - accuracy: 0.9912 - val_loss: 0.3721 - val_d_coef: 0.6322 - val_accuracy: 0.9917 - lr: 1.0000e-03\n",
      "Epoch 91/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4646 - d_coef: 0.5354 - accuracy: 0.9912\n",
      "Epoch 91: val_loss improved from 0.37153 to 0.37051, saving model to files\\model.h5\n",
      "32/32 [==============================] - 62s 2s/step - loss: 0.4646 - d_coef: 0.5354 - accuracy: 0.9912 - val_loss: 0.3705 - val_d_coef: 0.6338 - val_accuracy: 0.9918 - lr: 1.0000e-03\n",
      "Epoch 92/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4684 - d_coef: 0.5316 - accuracy: 0.9913\n",
      "Epoch 92: val_loss did not improve from 0.37051\n",
      "32/32 [==============================] - 62s 2s/step - loss: 0.4684 - d_coef: 0.5316 - accuracy: 0.9913 - val_loss: 0.3710 - val_d_coef: 0.6333 - val_accuracy: 0.9919 - lr: 1.0000e-03\n",
      "Epoch 93/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4636 - d_coef: 0.5364 - accuracy: 0.9913\n",
      "Epoch 93: val_loss did not improve from 0.37051\n",
      "32/32 [==============================] - 62s 2s/step - loss: 0.4636 - d_coef: 0.5364 - accuracy: 0.9913 - val_loss: 0.3739 - val_d_coef: 0.6305 - val_accuracy: 0.9916 - lr: 1.0000e-03\n",
      "Epoch 94/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4653 - d_coef: 0.5347 - accuracy: 0.9912\n",
      "Epoch 94: val_loss did not improve from 0.37051\n",
      "32/32 [==============================] - 61s 2s/step - loss: 0.4653 - d_coef: 0.5347 - accuracy: 0.9912 - val_loss: 0.3750 - val_d_coef: 0.6294 - val_accuracy: 0.9916 - lr: 1.0000e-03\n",
      "Epoch 95/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4645 - d_coef: 0.5355 - accuracy: 0.9912\n",
      "Epoch 95: val_loss improved from 0.37051 to 0.36994, saving model to files\\model.h5\n",
      "32/32 [==============================] - 61s 2s/step - loss: 0.4645 - d_coef: 0.5355 - accuracy: 0.9912 - val_loss: 0.3699 - val_d_coef: 0.6344 - val_accuracy: 0.9918 - lr: 1.0000e-03\n",
      "Epoch 96/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4662 - d_coef: 0.5338 - accuracy: 0.9912\n",
      "Epoch 96: val_loss improved from 0.36994 to 0.36901, saving model to files\\model.h5\n",
      "32/32 [==============================] - 62s 2s/step - loss: 0.4662 - d_coef: 0.5338 - accuracy: 0.9912 - val_loss: 0.3690 - val_d_coef: 0.6353 - val_accuracy: 0.9918 - lr: 1.0000e-03\n",
      "Epoch 97/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4651 - d_coef: 0.5349 - accuracy: 0.9912\n",
      "Epoch 97: val_loss improved from 0.36901 to 0.36879, saving model to files\\model.h5\n",
      "32/32 [==============================] - 62s 2s/step - loss: 0.4651 - d_coef: 0.5349 - accuracy: 0.9912 - val_loss: 0.3688 - val_d_coef: 0.6355 - val_accuracy: 0.9918 - lr: 1.0000e-03\n",
      "Epoch 98/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4673 - d_coef: 0.5327 - accuracy: 0.9912\n",
      "Epoch 98: val_loss improved from 0.36879 to 0.36862, saving model to files\\model.h5\n",
      "32/32 [==============================] - 62s 2s/step - loss: 0.4673 - d_coef: 0.5327 - accuracy: 0.9912 - val_loss: 0.3686 - val_d_coef: 0.6357 - val_accuracy: 0.9918 - lr: 1.0000e-03\n",
      "Epoch 99/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4634 - d_coef: 0.5366 - accuracy: 0.9912\n",
      "Epoch 99: val_loss improved from 0.36862 to 0.36832, saving model to files\\model.h5\n",
      "32/32 [==============================] - 62s 2s/step - loss: 0.4634 - d_coef: 0.5366 - accuracy: 0.9912 - val_loss: 0.3683 - val_d_coef: 0.6360 - val_accuracy: 0.9919 - lr: 1.0000e-03\n",
      "Epoch 100/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4653 - d_coef: 0.5347 - accuracy: 0.9913\n",
      "Epoch 100: val_loss improved from 0.36832 to 0.36746, saving model to files\\model.h5\n",
      "32/32 [==============================] - 62s 2s/step - loss: 0.4653 - d_coef: 0.5347 - accuracy: 0.9913 - val_loss: 0.3675 - val_d_coef: 0.6368 - val_accuracy: 0.9919 - lr: 1.0000e-03\n"
     ]
    }
   ],
   "source": [
    "\"\"\" UNETR  Configration \"\"\"\n",
    "o = {}\n",
    "o[\"i_size\"] = 256\n",
    "o[\"n_channels\"] = 3\n",
    "o[\"n_layers\"] = 12\n",
    "o[\"hidden_dim\"] = 128\n",
    "o[\"mp_dim\"] = 64\n",
    "o[\"n_heads\"] = 3\n",
    "o[\"dropout_rate\"] = 0.1\n",
    "o[\"patch_s\"] = 16\n",
    "o[\"n_patches\"] = (o[\"i_size\"]**2)//(o[\"patch_s\"]**2)\n",
    "o[\"flat_patches_shape\"] = (\n",
    "    o[\"n_patches\"],\n",
    "    o[\"patch_s\"]*o[\"patch_s\"]*o[\"n_channels\"]\n",
    ")\n",
    "\n",
    "def create_dir(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "def load_dataset(root_path, split=0.1):\n",
    "    \"\"\"\n",
    "    Load images and masks from a directory structure where\n",
    "    each subdirectory contains 'images' and 'masks' folders.\n",
    "\n",
    "    Args:\n",
    "        root_path (str): The root path containing the dataset (e.g., \"Brain_HE\").\n",
    "        split (float): Proportion of data to use for validation and test splits.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Train, validation, and test datasets (each as a tuple of image and mask paths).\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    masks = []\n",
    "\n",
    "    # Traverse the folder structure\n",
    "    for subdir in os.listdir(root_path):\n",
    "        subdir_path = os.path.join(root_path, subdir)\n",
    "        if os.path.isdir(subdir_path):\n",
    "            images_path = os.path.join(subdir_path, \"image\")\n",
    "            masks_path = os.path.join(subdir_path, \"mask\")\n",
    "            \n",
    "            if os.path.exists(images_path) and os.path.exists(masks_path):\n",
    "                images += sorted(glob(os.path.join(images_path, \"*.jpg\")))\n",
    "                masks += sorted(glob(os.path.join(masks_path, \"*.jpg\")))\n",
    "\n",
    "    # Ensure images and masks are matched\n",
    "    if len(images) != len(masks):\n",
    "        raise ValueError(\"Number of images and masks do not match!\")\n",
    "\n",
    "    # Split the data\n",
    "    split_size = int(len(images) * split)\n",
    "    train_x, valid_x = train_test_split(images, test_size=split_size, random_state=42)\n",
    "    train_y, valid_y = train_test_split(masks, test_size=split_size, random_state=42)\n",
    "\n",
    "    train_x, test_x = train_test_split(train_x, test_size=split_size, random_state=42)\n",
    "    train_y, test_y = train_test_split(train_y, test_size=split_size, random_state=42)\n",
    "\n",
    "    return (train_x, train_y), (valid_x, valid_y), (test_x, test_y)\n",
    "\n",
    "def read_image(path):\n",
    "    path = path.decode()\n",
    "    image = cv2.imread(path, cv2.IMREAD_COLOR)\n",
    "    image = cv2.resize(image, (o[\"i_size\"], o[\"i_size\"]))\n",
    "    image = image / 255.0\n",
    "\n",
    "    \"\"\" Processing to patches \"\"\"\n",
    "    patch_shape = (o[\"patch_s\"], o[\"patch_s\"], o[\"n_channels\"])\n",
    "    patches = patchify(image, patch_shape, o[\"patch_s\"])\n",
    "    patches = np.reshape(patches, o[\"flat_patches_shape\"])\n",
    "    patches = patches.astype(np.float32)\n",
    "\n",
    "    return patches\n",
    "\n",
    "def read_mask(path):\n",
    "    path = path.decode()\n",
    "    mask = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "    mask = cv2.resize(mask, (o[\"i_size\"], o[\"i_size\"]))\n",
    "    mask = mask / 255.0\n",
    "    mask = mask.astype(np.float32)\n",
    "    mask = np.expand_dims(mask, axis=-1)\n",
    "    return mask\n",
    "\n",
    "def tf_parse(x, y):\n",
    "    def _parse(x, y):\n",
    "        x = read_image(x)\n",
    "        y = read_mask(y)\n",
    "        return x, y\n",
    "\n",
    "    x, y = tf.numpy_function(_parse, [x, y], [tf.float32, tf.float32])\n",
    "    x.set_shape(o[\"flat_patches_shape\"])\n",
    "    y.set_shape([o[\"i_size\"], o[\"i_size\"], 1])\n",
    "    return x, y\n",
    "\n",
    "def tf_dataset(X, Y, batch=2):\n",
    "    ds = tf.data.Dataset.from_tensor_slices((X, Y))\n",
    "    ds = ds.map(tf_parse).batch(batch).prefetch(10)\n",
    "    return ds\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\" Seeding \"\"\"\n",
    "    np.random.seed(42)\n",
    "    tf.random.set_seed(42)\n",
    "\n",
    "    \"\"\" Directory for storing files \"\"\"\n",
    "    create_dir(\"files\")\n",
    "\n",
    "    \"\"\" Hyperparameters \"\"\"\n",
    "    batch_size = 8\n",
    "    lr = 0.1\n",
    "    n_epochs = 100\n",
    "    model_path = os.path.join(\"files\", \"model.h5\")\n",
    "    csv_path = os.path.join(\"files\", \"log.csv\")\n",
    "\n",
    "    \"\"\" Dataset \"\"\"\n",
    "    dataset_path = \"Brain HE\"\n",
    "    (train_x, train_y), (valid_x, valid_y), (test_x, test_y) = load_dataset(dataset_path)\n",
    "\n",
    "    print(f\"Train: \\t{len(train_x)} - {len(train_y)}\")\n",
    "    print(f\"Valid: \\t{len(valid_x)} - {len(valid_y)}\")\n",
    "    print(f\"Test: \\t{len(test_x)} - {len(test_y)}\")\n",
    "\n",
    "    train_dataset = tf_dataset(train_x, train_y, batch=batch_size)\n",
    "    valid_dataset = tf_dataset(valid_x, valid_y, batch=batch_size)\n",
    "\n",
    "    \"\"\" Model \"\"\"\n",
    "    model = build_unenr_2d(o)\n",
    "    model.compile(loss=d_loss, optimizer=SGD(lr), metrics=[d_coef, \"accuracy\"])\n",
    "    # model.summary()\n",
    "\n",
    "    callbacks = [\n",
    "        ModelCheckpoint(model_path, verbose=1, save_best_only=True),\n",
    "        ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, min_lr=1e-7, verbose=1),\n",
    "        CSVLogger(csv_path),\n",
    "        EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=False)\n",
    "    ]\n",
    "\n",
    "    model.fit(\n",
    "        train_dataset,\n",
    "        epochs=n_epochs,\n",
    "        validation_data=valid_dataset,\n",
    "        callbacks=callbacks\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bb3845-c663-4cc8-9cc3-c2fac90708aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6131d99a-0b97-451f-b2a2-e654f291e3c2",
   "metadata": {},
   "source": [
    "### Test results and corresponding image saving in 3 grid form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3892f384-ceae-4483-8491-1a97aa5d4683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: \t256 - 256\n",
      "Valid: \t31 - 31\n",
      "Test: \t31 - 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 31/31 [00:07<00:00,  4.21it/s]\n"
     ]
    }
   ],
   "source": [
    "\"\"\" UNETR  Configration \"\"\"\n",
    "# o = {}\n",
    "# o[\"image_size\"] = 256\n",
    "# o[\"num_channels\"] = 3\n",
    "# o[\"num_layers\"] = 12\n",
    "# o[\"hidden_dim\"] = 128\n",
    "# o[\"mlp_dim\"] = 32\n",
    "# cf[\"num_heads\"] = 2\n",
    "# cf[\"dropout_rate\"] = 0.1\n",
    "# cf[\"patch_size\"] = 16\n",
    "# cf[\"num_patches\"] = (cf[\"image_size\"]**2)//(cf[\"patch_size\"]**2)\n",
    "# cf[\"flat_patches_shape\"] = (\n",
    "#     cf[\"num_patches\"],\n",
    "#     cf[\"patch_size\"]*cf[\"patch_size\"]*cf[\"num_channels\"]\n",
    "# )\n",
    "o = {}\n",
    "o[\"i_size\"] = 256\n",
    "o[\"n_channels\"] = 3\n",
    "o[\"n_layers\"] = 12\n",
    "o[\"hidden_dim\"] = 128\n",
    "o[\"mp_dim\"] = 64\n",
    "o[\"n_heads\"] = 3\n",
    "o[\"dropout_rate\"] = 0.1\n",
    "o[\"patch_s\"] = 16\n",
    "o[\"n_patches\"] = (o[\"i_size\"]**2)//(o[\"patch_s\"]**2)\n",
    "o[\"flat_patches_shape\"] = (\n",
    "    o[\"n_patches\"],\n",
    "    o[\"patch_s\"]*o[\"patch_s\"]*o[\"n_channels\"]\n",
    ")\n",
    "\n",
    "get_custom_objects().update({'d_coef': d_coef})\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\" Seeding \"\"\"\n",
    "    np.random.seed(42)\n",
    "    tf.random.set_seed(42)\n",
    "\n",
    "    \"\"\" Directory for storing files \"\"\"\n",
    "    create_dir(f\"results1\")\n",
    "\n",
    "    \"\"\" Load the model \"\"\"\n",
    "    model_path = os.path.join(\"files\", \"model.h5\")\n",
    "    model = tf.keras.models.load_model(model_path, custom_objects={\"d_loss\": d_loss, \"dice_coef\": d_coef})\n",
    "\n",
    "    \"\"\" Dataset \"\"\"\n",
    "    dataset_path = \"Brain HE\"\n",
    "    (train_x, train_y), (valid_x, valid_y), (test_x, test_y) = load_dataset(dataset_path)\n",
    "\n",
    "    print(f\"Train: \\t{len(train_x)} - {len(train_y)}\")\n",
    "    print(f\"Valid: \\t{len(valid_x)} - {len(valid_y)}\")\n",
    "    print(f\"Test: \\t{len(test_x)} - {len(test_y)}\")\n",
    "\n",
    "    \"\"\" Prediction \"\"\"\n",
    "    for x, y in tqdm(zip(test_x, test_y), total=len(test_x)):\n",
    "        \"\"\" Extracting the name \"\"\"\n",
    "        name = x.split(\"/\")[-1]\n",
    "\n",
    "        \"\"\" Reading the image \"\"\"\n",
    "        image = cv2.imread(x, cv2.IMREAD_COLOR)\n",
    "        image = cv2.resize(image, (o[\"i_size\"], o[\"i_size\"]))\n",
    "        x = image / 255.0\n",
    "\n",
    "        patch_shape = (o[\"patch_s\"], o[\"patch_s\"], o[\"n_channels\"])\n",
    "        patches = patchify(x, patch_shape, o[\"patch_s\"])\n",
    "        patches = np.reshape(patches, o[\"flat_patches_shape\"])\n",
    "        patches = patches.astype(np.float32)\n",
    "        patches = np.expand_dims(patches, axis=0)\n",
    "\n",
    "        \"\"\" Read Mask \"\"\"\n",
    "        mask = cv2.imread(y, cv2.IMREAD_GRAYSCALE)\n",
    "        mask = cv2.resize(mask, (o[\"i_size\"], o[\"i_size\"]))\n",
    "        mask = mask / 255.0\n",
    "        mask = np.expand_dims(mask, axis=-1)\n",
    "        mask = np.concatenate([mask, mask, mask], axis=-1)\n",
    "\n",
    "        \"\"\" Prediction \"\"\"\n",
    "        pred = model.predict(patches, verbose=0)[0]\n",
    "        pred = np.concatenate([pred, pred, pred], axis=-1)\n",
    "\n",
    "        \"\"\" Save final mask \"\"\"\n",
    "        line = np.ones((o[\"i_size\"], 10, 3)) * 255\n",
    "        cat_images = np.concatenate([image, line, mask*255, line, pred*255], axis=1)\n",
    "        save_image_path = os.path.join(\"results2\",  name)\n",
    "        cv2.imwrite(save_image_path, cat_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3559a076-de33-410c-8e2c-222f6c1dfc45",
   "metadata": {},
   "source": [
    "### Test results by showing it localization with color mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6b1e7c46-84ea-41af-bbb7-9fa95c998a8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: \t256 - 256\n",
      "Valid: \t31 - 31\n",
      "Test: \t31 - 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 31/31 [00:06<00:00,  5.04it/s]\n"
     ]
    }
   ],
   "source": [
    "\"\"\" UNETR  Configration \"\"\"\n",
    "# cf = {}\n",
    "# cf[\"image_size\"] = 256\n",
    "# cf[\"num_channels\"] = 3\n",
    "# cf[\"num_layers\"] = 12\n",
    "# cf[\"hidden_dim\"] = 128\n",
    "# cf[\"mlp_dim\"] = 32\n",
    "# cf[\"num_heads\"] = 2\n",
    "# cf[\"dropout_rate\"] = 0.1\n",
    "# cf[\"patch_size\"] = 16\n",
    "# cf[\"num_patches\"] = (cf[\"image_size\"]**2)//(cf[\"patch_size\"]**2)\n",
    "# cf[\"flat_patches_shape\"] = (\n",
    "#     cf[\"num_patches\"],\n",
    "#     cf[\"patch_size\"]*cf[\"patch_size\"]*cf[\"num_channels\"]\n",
    "# )\n",
    "\n",
    "o = {}\n",
    "o[\"i_size\"] = 256\n",
    "o[\"n_channels\"] = 3\n",
    "o[\"n_layers\"] = 12\n",
    "o[\"hidden_dim\"] = 128\n",
    "o[\"mp_dim\"] = 64\n",
    "o[\"n_heads\"] = 2\n",
    "o[\"dropout_rate\"] = 0.1\n",
    "o[\"patch_s\"] = 16\n",
    "o[\"n_patches\"] = (o[\"i_size\"]**2)//(o[\"patch_s\"]**2)\n",
    "o[\"flat_patches_shape\"] = (\n",
    "    o[\"n_patches\"],\n",
    "    o[\"patch_s\"]*o[\"patch_s\"]*o[\"n_channels\"]\n",
    ")\n",
    "\n",
    "get_custom_objects().update({'d_coef': d_coef})\n",
    "\n",
    "get_custom_objects().update({'d_loss': d_loss})\n",
    "\n",
    "from matplotlib import cm\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\" Seeding \"\"\"\n",
    "    np.random.seed(42)\n",
    "    tf.random.set_seed(42)\n",
    "\n",
    "    \"\"\" Directory for storing files \"\"\"\n",
    "    create_dir(f\"results2\")\n",
    "\n",
    "    \"\"\" Load the model \"\"\"\n",
    "    model_path = os.path.join(\"files\", \"model.h5\")\n",
    "    model = tf.keras.models.load_model(model_path, custom_objects={\"dice_loss\": d_loss, \"dice_coef\": d_coef})\n",
    "\n",
    "    \"\"\" Dataset \"\"\"\n",
    "    dataset_path = \"Brain HE\"\n",
    "    (train_x, train_y), (valid_x, valid_y), (test_x, test_y) = load_dataset(dataset_path)\n",
    "\n",
    "    print(f\"Train: \\t{len(train_x)} - {len(train_y)}\")\n",
    "    print(f\"Valid: \\t{len(valid_x)} - {len(valid_y)}\")\n",
    "    print(f\"Test: \\t{len(test_x)} - {len(test_y)}\")\n",
    "\n",
    "\n",
    "\n",
    "        # Helper function to create colored masks\n",
    "    def create_color_mask(mask, color):\n",
    "        \"\"\"\n",
    "        Convert a binary mask into a specific color overlay.\n",
    "        Args:\n",
    "            mask: Binary mask (values between 0 and 1).\n",
    "            color: Tuple of (R, G, B) values for the color.\n",
    "        Returns:\n",
    "            A mask in the specified color.\n",
    "        \"\"\"\n",
    "        colored_mask = np.zeros((mask.shape[0], mask.shape[1], 3), dtype=np.uint8)\n",
    "        for i in range(3):  # Assign the color to respective channels\n",
    "            colored_mask[:, :, i] = (mask * color[i]).astype(np.uint8)\n",
    "        return colored_mask\n",
    "    \n",
    "    \"\"\" Prediction \"\"\"\n",
    "    for x, y in tqdm(zip(test_x, test_y), total=len(test_x)):\n",
    "        \"\"\" Extracting the name \"\"\"\n",
    "        name = x.split(\"/\")[-1]\n",
    "    \n",
    "        \"\"\" Reading the image \"\"\"\n",
    "        image = cv2.imread(x, cv2.IMREAD_COLOR)\n",
    "        image = cv2.resize(image, (o[\"i_size\"], o[\"i_size\"]))\n",
    "        x = image / 255.0\n",
    "    \n",
    "        patch_shape = (o[\"patch_s\"], o[\"patch_s\"], o[\"n_channels\"])\n",
    "        patches = patchify(x, patch_shape, o[\"patch_s\"])\n",
    "        patches = np.reshape(patches, o[\"flat_patches_shape\"])\n",
    "        patches = patches.astype(np.float32)\n",
    "        patches = np.expand_dims(patches, axis=0)\n",
    "    \n",
    "        \"\"\" Read Mask \"\"\"\n",
    "        mask = cv2.imread(y, cv2.IMREAD_GRAYSCALE)\n",
    "        mask = cv2.resize(mask, (o[\"i_size\"], o[\"i_size\"]))\n",
    "        mask = mask / 255.0\n",
    "    \n",
    "        \"\"\" Prediction \"\"\"\n",
    "        pred = model.predict(patches, verbose=0)[0]\n",
    "        pred = pred.reshape((o[\"i_size\"], o[\"i_size\"]))\n",
    "        pred = (pred > 0.5).astype(np.float32)  # Threshold prediction for binary mask\n",
    "    \n",
    "        \"\"\" Create  Masks \"\"\"\n",
    "        ground_truth_colored = create_color_mask(mask, (0, 0, 255))  # Red for Ground Truth\n",
    "        prediction_colored = create_color_mask(pred, (0, 255, 0))   # Green for Prediction\n",
    "    \n",
    "    \n",
    "        ground_truth_colored = cv2.addWeighted(image, 0.3, ground_truth_colored, 0.7, 0)\n",
    "        prediction_colored = cv2.addWeighted(image, 0.3, prediction_colored, 0.7, 0)\n",
    "    \n",
    "        \"\"\" Combine  two masks \"\"\"\n",
    "        combined_overlay = cv2.addWeighted(ground_truth_colored, 0.5, prediction_colored, 0.5, 0)\n",
    "    \n",
    "        \"\"\" Save visualization \"\"\"\n",
    "        save_image_path = os.path.join(\"results2\", name)\n",
    "        cv2.imwrite(save_image_path, combined_overlay)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "689d3646-7b4a-48dd-8e6d-5824a1db847e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256\n",
      "31\n",
      "31\n",
      "256\n",
      "31\n",
      "31\n"
     ]
    }
   ],
   "source": [
    "# print(len(train_x))\n",
    "# print(len(valid_x))\n",
    "# print(len(test_x))\n",
    "\n",
    "# print(len(train_y))\n",
    "# print(len(valid_y))\n",
    "# print(len(test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a8e1ff-82b9-4c31-8101-95aaf15dbc0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd33f37-fc68-4cce-847d-5237ae3cc573",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
